# ==============================================
# API Configuration
# ==============================================
api:
  # Base URL for the job posts API endpoint.
  base_url: "https://api.karbord.io/api/v1/Candidate/JobPost/GetList"
  
  # HTTP headers for all API requests.
  headers:
    accept: "application/json, text/plain, */*"
    accept-encoding: "gzip, deflate, br, zstd"
    accept-language: "en-US,en;q=0.9"
    # Unique client identifier; can be overridden by environment variables if needed.
    clientid: "4575772"
    content-type: "application/json"
    ngsw-bypass: "true"
    origin: "https://karbord.io"
    referer: "https://karbord.io/"
    sec-ch-ua: '"Not(A:Brand";v="99", "Google Chrome";v="133", "Chromium";v="133"'
    sec-ch-ua-mobile: "?0"
    sec-ch-ua-platform: '"Windows"'
    sec-fetch-dest: "empty"
    sec-fetch-mode: "cors"
    sec-fetch-site: "same-site"
    user-agent: "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/133.0.0.0 Safari/537.36"

# ==============================================
# Default Request Payload
# ==============================================
request:
  default_payload:
    isInternship: false
    isRemote: false
    location: null
    publishDate: null
    workType: null
    pageSize: 100         # Number of items to request per page.
    sort: 0
    searchId: null
    jobPostCategories: [] # Categories for filtering job posts.
    jobBoardIds: []       # List of job board IDs to filter by.
    hasNoWorkExperienceRequirement: false
    clientId: 4558668     # Must match the API's expected client ID.
    page: 1               # Default starting page.
    nextPageToken: null   # Token for pagination if required.

# ==============================================
# Scraper Configuration
# ==============================================
scraper:

  # ==============================================
# Database Configuration
# ==============================================
  database:
    enabled: true
    # Connection string will be built from POSTGRES_* environment variables
    connection_string: ""
    schema: "public"
    batch_size: 1000         # Maximum number of jobs per bulk insert.
    save_raw_data: true      # Option to also save raw data locally.
    max_connections: 10
    retry_attempts: 3
    retry_delay: 5           # Delay (in seconds) between retry attempts.

  # Batch and Pagination Settings
  batch_size: 100            # Number of jobs per API request (batch).
  jobs_per_batch: 1000       # Total jobs to accumulate before processing a full batch.
  max_pages: 9000            # Maximum number of pages to scrape in one run.
  chunk_size: 1000           # Chunk size when processing/inserting jobs.

  # Resource and Concurrency Settings
  memory_limit: 1024         # Maximum allowed memory usage in MB.
  max_concurrent_requests: 5 # Limit on simultaneous HTTP requests.
  timeout: 60                # HTTP request timeout in seconds.

  # Timing and Rate Limiting Settings
  sleep_time: 0.5            # Base delay between API requests (in seconds).
  rate_limit:
    requests_per_minute: 120 # Maximum number of API requests per minute.
    burst: 10                # Maximum burst (immediate) requests allowed.

  # Retry Logic
  max_retries: 5             # Maximum number of retries on failure.
  retry_delay:
    min: 2                   # Minimum delay before a retry (seconds).
    max: 10                  # Maximum delay before a retry (seconds).

  # Resume and Empty-Page Logic
  lookback_pages: 5          # Number of pages to look back when resuming.
  minimum_pages: 10          # Minimum pages to check before stopping.
  max_empty_pages: 3         # Maximum consecutive empty pages allowed.
  max_resume_age: 86400      # Maximum age (in seconds) of previous state to resume.

  # State Management
  state_tracking:
    enabled: true
    save_interval: 300       # Time interval (in seconds) to auto-save the state.
    backup_count: 3          # Number of state backups to retain.

  # Timestamp Formatting
  timestamp_settings:
    format: "%Y-%m-%dT%H:%M:%S.%fZ"  # Expected timestamp format.
    timezone: "UTC"

  # Job Deduplication
  deduplication:
    enabled: true
    method: "id_based"       # Deduplication strategy: use 'id_based' or 'content_based'.
    cache_size: 10000        # Maximum number of job IDs to keep in memory.

  # Monitoring and Alerts
  monitoring:
    enabled: true
    metrics:
      - new_jobs_count     # Count of new jobs found.
      - processing_time    # Time taken for processing.
      - memory_usage       # Memory usage statistics.
      - request_latency    # Latency of API requests.
    alert_thresholds:
      max_errors_per_minute: 10  # Threshold for error alerts.
      max_memory_percent: 90     # Maximum memory usage percentage.
      max_latency_ms: 5000       # Maximum allowed latency in milliseconds.

  # Data Validation
  validation:
    enabled: true
    required_fields:
      - id
      - title
      - posted_at         # Ensure each job contains these fields.

  # Output Settings (for local backups)
  output:
    format: "json"           # File format for output (e.g., JSON).
    compression: true        # Whether to compress output files.
    batch_prefix: "batch_"   # Prefix used for naming batch files.
    timestamp_format: "%Y%m%d_%H%M%S"  # Timestamp format in filenames.

  # Cleanup Settings (for temporary files)
  cleanup:
    enabled: true
    max_age_days: 7          # Remove files older than 7 days.
    keep_last_n_batches: 50  # Always keep the most recent 50 batches.

  # Debug Options
  debug:
    enabled: true
    verbose_logging: true
    save_raw_responses: true

